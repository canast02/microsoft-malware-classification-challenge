import os
from csv import writer
from sklearn import cross_validation

from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier
import six

import utilities


# Decide read/write mode based on python version
read_mode, write_mode = ('r', 'w') if six.PY2 else ('rt', 'wt')

# Set the path to your consolidated files
path = '/Users/chrysovalantis/Documents/UCY/EPL451/Project'
os.chdir(path)

# File names
ftrain = 'train_consolidation.txt'
ftest = 'test_consolidation.txt'
flabel = 'trainLabels.csv'
fsubmission = 'submission.csv'

labels = utilities.read_labels(flabel)

# Dimensions for train set
ntrain = 10868
nfeature = 16 ** 2 + 1 + 1  # For two_byte_codes, no_que_marks, label
train = utilities.read_train(ntrain, nfeature, labels, ftrain)

X = train[:, :-1]
y = train[:,  -1]

del labels
del train

# Parameters for trees
random_state = 5342
n_jobs = 8
verbose = 1
n_estimators = 89
# ExtraTreesClassifier - classifier 1
clf1 = ExtraTreesClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
clf2 = ExtraTreesClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
# RandomForestClassifier - classifier 2
n_estimators = 89
clf3 = RandomForestClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
clf4 = RandomForestClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
# AdaBoostClassifier - classifier 3
estimator = ExtraTreesClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
clf5 = AdaBoostClassifier(base_estimator=estimator, random_state=random_state, learning_rate=0.8)

# Start training
print('training started')
clf1.fit(X, y)
X_new1 = clf1.transform(X, '1.25*median')
clf3.fit(X, y)
X_new2 = clf3.transform(X)

print('n_components = ', len(X_new1[0]), len(X_new2[0]))

# ############################
# # test log loss
# print('computing log loss')
# kf = cross_validation.KFold(ntrain, n_folds=4)
#
# _logloss = 0.0
# for trainIndex, testIndex in kf:
#     print("TRAIN:", trainIndex, "TEST:", testIndex)
#     X_train1, X_test1 = X_new1[trainIndex], X_new1[testIndex]
#     X_train2, X_test2 = X_new2[trainIndex], X_new2[testIndex]
#     y_train, y_test = y[trainIndex], y[testIndex]
#
#     clf2.fit(X_train1, y_train)
#     clf4.fit(X_train2, y_train)
#     clf5.fit(X_train1, y_train)
#
#     pred1 = clf2.predict_proba(X_test1)
#     pred2 = clf4.predict_proba(X_test2)
#     pred3 = clf5.predict_proba(X_test1)
#
#     pred = utilities.avg_proba3(pred1, pred2, pred3)
#
#     _logloss += utilities.log_loss(pred, y_test)
#
# print('log loss = ', _logloss/len(kf))
# ############################

clf2.fit(X_new1, y)
clf4.fit(X_new2, y)
clf5.fit(X_new1, y)
print('training completed')

# We don't need training set now
del X_new1
del X_new2

# Dimensions for train set
ntest = 10873
nfeature = 16 ** 2 + 1  # For two_byte_codes, no_que_marks
test, Ids = utilities.read_test(ntest, nfeature, ftest)

test_new1 = clf1.transform(test, '1.25*median')
test_new2 = clf3.transform(test)

del test

# Predict for whole test set
pred1 = clf2.predict_proba(test_new1)
pred2 = clf4.predict_proba(test_new2)
pred3 = clf5.predict_proba(test_new1)

# calculate the average probabilities
final_pred = utilities.avg_proba3(pred1, pred2, pred3)

# Writing results to file
with open(fsubmission, write_mode) as f:
    fw = writer(f)
    # Header preparation
    header = ['Id'] + ['Prediction' + str(i) for i in range(1, 10)]
    fw.writerow(header)
    for t, (Id, pred) in enumerate(zip(Ids, final_pred.tolist())):
        fw.writerow([Id] + pred)
        if (t + 1) % 1000 == 0:
            print(t + 1, 'prediction written')

print('all done!')