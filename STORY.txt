Approach 1: (only .bytes)

Naive bayes -> Best: 1.84, 2byte codes, includes question marks
- 4byte codes had worse performance + too many combinations made the whole process inefficient
- naive bayes is probabilistic and the large number of features caused probabilities to go very low (underflow)
* scala programming language, had to implement the algorithm, too much time

Approach 2: (only .bytes)

RandomForestTree -> Best: 0.154290812, 2byte codes, includes question marks
- 4byte codes increased log loss
- without question marks -> 0.179632309
* moved to python, already have many ML algorithms implemented, take advantage of them

Approach 3: (only .bytes)

RandomForestTree & ExtraTreeClassifier -> Best: 0.101819042, 2 byte codes, includes question marks
- averaging the predictions of two different approaches

Approach 4: (only .bytes)

RandomForestTree & ExtraTreeClassifier & Dimensionality Reduction (Feature selection) -> Best: 0.059852499, 2 byte codes, includes question marks
- reduced features to 110 & 24 respectively
- was now able to efficiently consider all features at tree node split

Approach 5: (only .bytes)

RandomForestTree & ExtraTreeClassifier & Dimensionality Reduction (Feature selection) & Boost -> Best: 0.058842222
- subsequent trees are built using different weight on previously misclassified instances